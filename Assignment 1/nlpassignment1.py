# -*- coding: utf-8 -*-
"""NLPAssignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19RcFr0qc6QW0H7LoSuP1jtfKF43cMg2i
"""

pip install datasets

from datasets import load_dataset

dataset = load_dataset(
    "ai4bharat/IndicCorpV2",
    name="indiccorp_v2",
    split="hin_Deva",
    streaming=True  # Optional: avoid full download
)

print(next(iter(dataset)))

def custom_sentence_tokenizer(text):
    # Split by Hindi sentence enders: । ? !
    # Keep punctuation attached to sentence
    sentences = re.split(r'(?<=[।!,?])\s+', text.strip())
    return [s.strip() for s in sentences if s.strip()]

import re

def custom_word_tokenizer(text):
    pattern = (
        r'\b[\w\.-]+@[\w\.-]+\.\w+\b'       # emails
        r'|https?://\S+'                    # URLs
        r'|\d+\.\d+|\d+'                    # numbers
        r'|[\u0900-\u097F]+'                # Hindi words (matras included)
        r'|[^\s\w\u0900-\u097F]'            # punctuation
    )
    return re.findall(pattern, text)

from datasets import load_dataset

dataset = load_dataset("ai4bharat/IndicCorpV2", name="indiccorp_v2", split="hin_Deva", streaming=True)

total_sentences = 0
total_words = 0
total_chars = 0
unique_words = set()

MAX_PARAGRAPHS = 500_000  # Optional limit for speed

for i, example in enumerate(dataset):
    text = example["text"]

    # Custom sentence tokenizer
    sentences = custom_sentence_tokenizer(text)
    for sent in sentences:
        total_sentences += 1
        words = custom_word_tokenizer(sent)
        total_words += len(words)
        total_chars += sum(len(word) for word in words)
        unique_words.update(words)

    if i + 1 >= MAX_PARAGRAPHS:
        break

# Final statistics
print(f"Total sentences: {total_sentences}")
print(f"Total words: {total_words}")
print(f"Total characters: {total_chars}")
print(f"Average sentence length (words/sentence): {total_words / total_sentences:.2f}")
print(f"Average word length (characters/word): {total_chars / total_words:.2f}")
print(f"Type/Token Ratio: {len(unique_words) / total_words:.4f}")

import itertools

# Take first 3 paragraphs from the streaming dataset
for i, example in enumerate(itertools.islice(dataset, 3), start=1):
    text = example["text"].strip()
    print(f"\n=== Row {i} Original Text ===")
    print(text)

    # Sentence Tokenization
    sentences = custom_sentence_tokenizer(text)
    print("\n--- Sentences ---")
    for idx, sent in enumerate(sentences, 1):
        print(f"{idx}. {sent}")

    # Word Tokenization for the first sentence
    if sentences:
        print("\n--- Words in Sentence 1 ---")
        words = custom_word_tokenizer(sentences[0])
        print(words)
    else:
        print("\n(No valid sentences found in this row)")

# Save tokenized sentences into a file
output_file = "tokenized_sentences.txt"

total_sentences = 0
total_words = 0
total_chars = 0
unique_words = set()
MAX_PARAGRAPHS = 500000  # Process first 5 lakh paragraphs (adjust if needed)

with open(output_file, "w", encoding="utf-8") as f:
    for i, example in enumerate(dataset):
        text = example["text"].strip()
        sentences = custom_sentence_tokenizer(text)

        for sent in sentences:
            words = custom_word_tokenizer(sent)
            if not words:
                continue

            tokenized_sentence = " ".join(words)
            f.write(tokenized_sentence + "\n")

            total_sentences += 1
            total_words += len(words)
            total_chars += sum(len(word) for word in words)
            unique_words.update(words)

        if i + 1 >= MAX_PARAGRAPHS:  # Stop after 5 lakh paragraphs
            break

print(f"✅ Tokenized sentences saved to {output_file}")

print(f"Total sentences: {total_sentences}")
print(f"Total words: {total_words}")
print(f"Total characters: {total_chars}")
print(f"Average sentence length: {total_words / total_sentences:.2f} words")
print(f"Average word length: {total_chars / total_words:.2f} characters")
print(f"Type/Token Ratio: {len(unique_words) / total_words:.4f}")